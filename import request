import requests
from bs4 import BeautifulSoup

def get_html_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.content
    else:
        return None

def extract_title(soup):
    return soup.find('h1', {'id': 'firstHeading'}).text

def extract_article_text(soup):
    content = {}
    for header in soup.find_all(['h2', 'h3']):
        heading = header.text.strip()
        paragraphs = []
        for sibling in header.find_next_siblings():
            if sibling.name in ['h2', 'h3']:
                break
            if sibling.name == 'p':
                paragraphs.append(sibling.text.strip())
        if paragraphs:
            content[heading] = paragraphs
    return content

def collect_internal_links(soup):
    base_url = 'https://en.wikipedia.org'
    links = set()
    for link in soup.find_all('a', href=True):
        href = link['href']
        if href.startswith('/wiki/') and not ':' in href:
            full_url = base_url + href
            links.add(full_url)
    return links

def process_wikipedia_page(url):
    html_content = get_html_content(url)
    if html_content:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = extract_title(soup)
        article_text = extract_article_text(soup)
        internal_links = collect_internal_links(soup)
        
        return {
            'title': title,
            'text': article_text,
            'links': internal_links
        }
    else:
        return None

url = 'https://en.wikipedia.org/wiki/Web_scraping'
result = process_wikipedia_page(url)

print("Title:", result['title'])
print("\nArticle Text:", result['text'])
print("\nInternal Links:", result['links'])

import requests
from bs4 import BeautifulSoup

# 1.1 Get and Parse HTML Content
def get_html_content(url):
    response = requests.get(url)
    if response.status_code == 200:
        return response.content
    else:
        return None

# 1.2 Extract Article Title
def extract_title(soup):
    return soup.find('h1', {'id': 'firstHeading'}).text

# 1.3 Extract Article Text with Headings
def extract_article_text(soup):
    content = {}
    for header in soup.find_all(['h2', 'h3']):
        heading = header.text.strip()
        paragraphs = []
        for sibling in header.find_next_siblings():
            if sibling.name in ['h2', 'h3']:
                break
            if sibling.name == 'p':
                paragraphs.append(sibling.text.strip())
        if paragraphs:
            content[heading] = paragraphs
    return content

# 1.4 Collect Links to Other Wikipedia Pages
def collect_internal_links(soup):
    base_url = 'https://en.wikipedia.org'
    links = set()
    for link in soup.find_all('a', href=True):
        href = link['href']
        if href.startswith('/wiki/') and not ':' in href:
            full_url = base_url + href
            links.add(full_url)
    return links

# 1.5 Wrap All Functions
def process_wikipedia_page(url):
    html_content = get_html_content(url)
    if html_content:
        soup = BeautifulSoup(html_content, 'html.parser')
        
        title = extract_title(soup)
        article_text = extract_article_text(soup)
        internal_links = collect_internal_links(soup)
        
        return {
            'title': title,
            'text': article_text,
            'links': internal_links
        }
    else:
        return None

# 1.6 Test the Final Function
url = 'https://en.wikipedia.org/wiki/Web_scraping'
result = process_wikipedia_page(url)

print("Title:", result['title'])
print("\nArticle Text:", result['text'])
print("\nInternal Links:", result['links'])
